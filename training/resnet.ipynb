{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91069117",
   "metadata": {},
   "source": [
    "1. Test learning rates of 0.001, 0.0003, 0.0001\n",
    "2. Weight decay of 0, 0.0001, 0.001\n",
    "3. Batch size of 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd53a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.9.0+cu126\n",
      "cuda available: True\n",
      "gpu name: NVIDIA GeForce GTX 1650\n",
      "Using device: cuda:0\n",
      "\n",
      "=== Training with lr=0.001, weight_decay=0.0, batch_size=16 ===\n",
      "lr=0.001, wd=0.0, bs=16 | Epoch 1/5 | Loss: 0.0927 | F1: 0.9984 (P: 1.0000, R: 0.9967)\n",
      "lr=0.001, wd=0.0, bs=16 | Epoch 2/5 | Loss: 0.0171 | F1: 0.9993 (P: 1.0000, R: 0.9987)\n",
      "lr=0.001, wd=0.0, bs=16 | Epoch 3/5 | Loss: 0.0058 | F1: 0.9987 (P: 1.0000, R: 0.9974)\n",
      "lr=0.001, wd=0.0, bs=16 | Epoch 4/5 | Loss: 0.0082 | F1: 0.9993 (P: 0.9993, R: 0.9993)\n",
      "lr=0.001, wd=0.0, bs=16 | Epoch 5/5 | Loss: 0.0052 | F1: 1.0000 (P: 1.0000, R: 1.0000)\n",
      "\n",
      "=== Training with lr=0.001, weight_decay=0.0, batch_size=32 ===\n",
      "lr=0.001, wd=0.0, bs=32 | Epoch 1/5 | Loss: 0.0935 | F1: 0.9967 (P: 0.9980, R: 0.9954)\n",
      "lr=0.001, wd=0.0, bs=32 | Epoch 2/5 | Loss: 0.0179 | F1: 0.9993 (P: 1.0000, R: 0.9987)\n",
      "lr=0.001, wd=0.0, bs=32 | Epoch 3/5 | Loss: 0.0073 | F1: 0.9987 (P: 0.9993, R: 0.9980)\n",
      "lr=0.001, wd=0.0, bs=32 | Epoch 4/5 | Loss: 0.0067 | F1: 1.0000 (P: 1.0000, R: 1.0000)\n",
      "lr=0.001, wd=0.0, bs=32 | Epoch 5/5 | Loss: 0.0038 | F1: 1.0000 (P: 1.0000, R: 1.0000)\n",
      "\n",
      "=== Training with lr=0.001, weight_decay=0.0, batch_size=64 ===\n",
      "lr=0.001, wd=0.0, bs=64 | Epoch 1/5 | Loss: 0.1315 | F1: 0.9925 (P: 0.9961, R: 0.9889)\n",
      "lr=0.001, wd=0.0, bs=64 | Epoch 2/5 | Loss: 0.0268 | F1: 0.9954 (P: 0.9980, R: 0.9928)\n",
      "lr=0.001, wd=0.0, bs=64 | Epoch 3/5 | Loss: 0.0127 | F1: 0.9974 (P: 0.9987, R: 0.9961)\n",
      "lr=0.001, wd=0.0, bs=64 | Epoch 4/5 | Loss: 0.0082 | F1: 0.9990 (P: 0.9993, R: 0.9987)\n",
      "lr=0.001, wd=0.0, bs=64 | Epoch 5/5 | Loss: 0.0054 | F1: 0.9993 (P: 0.9993, R: 0.9993)\n",
      "\n",
      "=== Training with lr=0.001, weight_decay=0.0001, batch_size=16 ===\n",
      "lr=0.001, wd=0.0001, bs=16 | Epoch 1/5 | Loss: 0.0874 | F1: 0.9984 (P: 0.9987, R: 0.9980)\n",
      "lr=0.001, wd=0.0001, bs=16 | Epoch 2/5 | Loss: 0.0174 | F1: 0.9993 (P: 0.9993, R: 0.9993)\n",
      "lr=0.001, wd=0.0001, bs=16 | Epoch 3/5 | Loss: 0.0143 | F1: 1.0000 (P: 1.0000, R: 1.0000)\n",
      "lr=0.001, wd=0.0001, bs=16 | Epoch 4/5 | Loss: 0.0071 | F1: 1.0000 (P: 1.0000, R: 1.0000)\n",
      "lr=0.001, wd=0.0001, bs=16 | Epoch 5/5 | Loss: 0.0049 | F1: 1.0000 (P: 1.0000, R: 1.0000)\n",
      "\n",
      "=== Training with lr=0.001, weight_decay=0.0001, batch_size=32 ===\n",
      "lr=0.001, wd=0.0001, bs=32 | Epoch 1/5 | Loss: 0.0930 | F1: 0.9980 (P: 0.9980, R: 0.9980)\n",
      "lr=0.001, wd=0.0001, bs=32 | Epoch 2/5 | Loss: 0.0179 | F1: 0.9977 (P: 0.9993, R: 0.9961)\n",
      "lr=0.001, wd=0.0001, bs=32 | Epoch 3/5 | Loss: 0.0100 | F1: 0.9984 (P: 1.0000, R: 0.9967)\n",
      "lr=0.001, wd=0.0001, bs=32 | Epoch 4/5 | Loss: 0.0074 | F1: 1.0000 (P: 1.0000, R: 1.0000)\n",
      "lr=0.001, wd=0.0001, bs=32 | Epoch 5/5 | Loss: 0.0044 | F1: 1.0000 (P: 1.0000, R: 1.0000)\n",
      "\n",
      "=== Training with lr=0.001, weight_decay=0.0001, batch_size=64 ===\n",
      "lr=0.001, wd=0.0001, bs=64 | Epoch 1/5 | Loss: 0.1269 | F1: 0.9928 (P: 0.9941, R: 0.9915)\n",
      "lr=0.001, wd=0.0001, bs=64 | Epoch 2/5 | Loss: 0.0279 | F1: 0.9974 (P: 0.9961, R: 0.9987)\n",
      "lr=0.001, wd=0.0001, bs=64 | Epoch 3/5 | Loss: 0.0114 | F1: 0.9984 (P: 1.0000, R: 0.9967)\n",
      "lr=0.001, wd=0.0001, bs=64 | Epoch 4/5 | Loss: 0.0076 | F1: 0.9997 (P: 1.0000, R: 0.9993)\n",
      "lr=0.001, wd=0.0001, bs=64 | Epoch 5/5 | Loss: 0.0052 | F1: 0.9990 (P: 1.0000, R: 0.9980)\n",
      "\n",
      "=== Training with lr=0.001, weight_decay=0.001, batch_size=16 ===\n",
      "lr=0.001, wd=0.001, bs=16 | Epoch 1/5 | Loss: 0.0874 | F1: 0.9964 (P: 0.9993, R: 0.9935)\n",
      "lr=0.001, wd=0.001, bs=16 | Epoch 2/5 | Loss: 0.0227 | F1: 0.9974 (P: 1.0000, R: 0.9948)\n",
      "lr=0.001, wd=0.001, bs=16 | Epoch 3/5 | Loss: 0.0105 | F1: 0.9987 (P: 1.0000, R: 0.9974)\n",
      "lr=0.001, wd=0.001, bs=16 | Epoch 4/5 | Loss: 0.0081 | F1: 0.9990 (P: 1.0000, R: 0.9980)\n",
      "lr=0.001, wd=0.001, bs=16 | Epoch 5/5 | Loss: 0.0061 | F1: 0.9977 (P: 1.0000, R: 0.9954)\n",
      "\n",
      "=== Training with lr=0.001, weight_decay=0.001, batch_size=32 ===\n",
      "lr=0.001, wd=0.001, bs=32 | Epoch 1/5 | Loss: 0.0923 | F1: 0.9958 (P: 0.9980, R: 0.9935)\n",
      "lr=0.001, wd=0.001, bs=32 | Epoch 2/5 | Loss: 0.0174 | F1: 0.9980 (P: 0.9993, R: 0.9967)\n",
      "lr=0.001, wd=0.001, bs=32 | Epoch 3/5 | Loss: 0.0079 | F1: 0.9993 (P: 1.0000, R: 0.9987)\n",
      "lr=0.001, wd=0.001, bs=32 | Epoch 4/5 | Loss: 0.0045 | F1: 0.9993 (P: 1.0000, R: 0.9987)\n",
      "lr=0.001, wd=0.001, bs=32 | Epoch 5/5 | Loss: 0.0031 | F1: 0.9993 (P: 1.0000, R: 0.9987)\n",
      "\n",
      "=== Training with lr=0.001, weight_decay=0.001, batch_size=64 ===\n",
      "lr=0.001, wd=0.001, bs=64 | Epoch 1/5 | Loss: 0.1271 | F1: 0.9935 (P: 0.9941, R: 0.9928)\n",
      "lr=0.001, wd=0.001, bs=64 | Epoch 2/5 | Loss: 0.0271 | F1: 0.9967 (P: 0.9987, R: 0.9948)\n",
      "lr=0.001, wd=0.001, bs=64 | Epoch 3/5 | Loss: 0.0130 | F1: 0.9984 (P: 0.9993, R: 0.9974)\n",
      "lr=0.001, wd=0.001, bs=64 | Epoch 4/5 | Loss: 0.0082 | F1: 0.9990 (P: 0.9993, R: 0.9987)\n",
      "lr=0.001, wd=0.001, bs=64 | Epoch 5/5 | Loss: 0.0048 | F1: 0.9990 (P: 0.9993, R: 0.9987)\n",
      "\n",
      "=== Training with lr=0.0003, weight_decay=0.0, batch_size=16 ===\n",
      "lr=0.0003, wd=0.0, bs=16 | Epoch 1/5 | Loss: 0.1384 | F1: 0.9915 (P: 0.9948, R: 0.9883)\n",
      "lr=0.0003, wd=0.0, bs=16 | Epoch 2/5 | Loss: 0.0439 | F1: 0.9958 (P: 0.9961, R: 0.9954)\n",
      "lr=0.0003, wd=0.0, bs=16 | Epoch 3/5 | Loss: 0.0238 | F1: 0.9974 (P: 0.9993, R: 0.9954)\n",
      "lr=0.0003, wd=0.0, bs=16 | Epoch 4/5 | Loss: 0.0211 | F1: 0.9974 (P: 0.9993, R: 0.9954)\n",
      "lr=0.0003, wd=0.0, bs=16 | Epoch 5/5 | Loss: 0.0143 | F1: 0.9971 (P: 1.0000, R: 0.9941)\n",
      "\n",
      "=== Training with lr=0.0003, weight_decay=0.0, batch_size=32 ===\n",
      "lr=0.0003, wd=0.0, bs=32 | Epoch 1/5 | Loss: 0.1748 | F1: 0.9872 (P: 0.9954, R: 0.9791)\n",
      "lr=0.0003, wd=0.0, bs=32 | Epoch 2/5 | Loss: 0.0529 | F1: 0.9941 (P: 0.9967, R: 0.9915)\n",
      "lr=0.0003, wd=0.0, bs=32 | Epoch 3/5 | Loss: 0.0330 | F1: 0.9954 (P: 0.9967, R: 0.9941)\n",
      "lr=0.0003, wd=0.0, bs=32 | Epoch 4/5 | Loss: 0.0257 | F1: 0.9951 (P: 0.9987, R: 0.9915)\n",
      "lr=0.0003, wd=0.0, bs=32 | Epoch 5/5 | Loss: 0.0161 | F1: 0.9971 (P: 0.9987, R: 0.9954)\n",
      "\n",
      "=== Training with lr=0.0003, weight_decay=0.0, batch_size=64 ===\n",
      "lr=0.0003, wd=0.0, bs=64 | Epoch 1/5 | Loss: 0.2342 | F1: 0.9816 (P: 0.9907, R: 0.9726)\n",
      "lr=0.0003, wd=0.0, bs=64 | Epoch 2/5 | Loss: 0.0783 | F1: 0.9882 (P: 0.9934, R: 0.9831)\n",
      "lr=0.0003, wd=0.0, bs=64 | Epoch 3/5 | Loss: 0.0477 | F1: 0.9922 (P: 0.9935, R: 0.9909)\n",
      "lr=0.0003, wd=0.0, bs=64 | Epoch 4/5 | Loss: 0.0335 | F1: 0.9938 (P: 0.9974, R: 0.9902)\n",
      "lr=0.0003, wd=0.0, bs=64 | Epoch 5/5 | Loss: 0.0259 | F1: 0.9958 (P: 0.9974, R: 0.9941)\n",
      "\n",
      "=== Training with lr=0.0003, weight_decay=0.0001, batch_size=16 ===\n",
      "lr=0.0003, wd=0.0001, bs=16 | Epoch 1/5 | Loss: 0.1367 | F1: 0.9875 (P: 0.9967, R: 0.9785)\n",
      "lr=0.0003, wd=0.0001, bs=16 | Epoch 2/5 | Loss: 0.0465 | F1: 0.9944 (P: 0.9980, R: 0.9909)\n",
      "lr=0.0003, wd=0.0001, bs=16 | Epoch 3/5 | Loss: 0.0292 | F1: 0.9967 (P: 0.9993, R: 0.9941)\n",
      "lr=0.0003, wd=0.0001, bs=16 | Epoch 4/5 | Loss: 0.0190 | F1: 0.9961 (P: 0.9993, R: 0.9928)\n",
      "lr=0.0003, wd=0.0001, bs=16 | Epoch 5/5 | Loss: 0.0122 | F1: 0.9990 (P: 0.9993, R: 0.9987)\n",
      "\n",
      "=== Training with lr=0.0003, weight_decay=0.0001, batch_size=32 ===\n",
      "lr=0.0003, wd=0.0001, bs=32 | Epoch 1/5 | Loss: 0.1608 | F1: 0.9869 (P: 0.9927, R: 0.9811)\n",
      "lr=0.0003, wd=0.0001, bs=32 | Epoch 2/5 | Loss: 0.0550 | F1: 0.9945 (P: 0.9954, R: 0.9935)\n",
      "lr=0.0003, wd=0.0001, bs=32 | Epoch 3/5 | Loss: 0.0320 | F1: 0.9951 (P: 0.9980, R: 0.9922)\n",
      "lr=0.0003, wd=0.0001, bs=32 | Epoch 4/5 | Loss: 0.0231 | F1: 0.9974 (P: 0.9987, R: 0.9961)\n",
      "lr=0.0003, wd=0.0001, bs=32 | Epoch 5/5 | Loss: 0.0162 | F1: 0.9977 (P: 0.9993, R: 0.9961)\n",
      "\n",
      "=== Training with lr=0.0003, weight_decay=0.0001, batch_size=64 ===\n",
      "lr=0.0003, wd=0.0001, bs=64 | Epoch 1/5 | Loss: 0.2328 | F1: 0.9812 (P: 0.9914, R: 0.9713)\n",
      "lr=0.0003, wd=0.0001, bs=64 | Epoch 2/5 | Loss: 0.0776 | F1: 0.9892 (P: 0.9934, R: 0.9850)\n",
      "lr=0.0003, wd=0.0001, bs=64 | Epoch 3/5 | Loss: 0.0493 | F1: 0.9922 (P: 0.9948, R: 0.9896)\n",
      "lr=0.0003, wd=0.0001, bs=64 | Epoch 4/5 | Loss: 0.0334 | F1: 0.9945 (P: 0.9948, R: 0.9941)\n",
      "lr=0.0003, wd=0.0001, bs=64 | Epoch 5/5 | Loss: 0.0258 | F1: 0.9958 (P: 0.9954, R: 0.9961)\n",
      "\n",
      "=== Training with lr=0.0003, weight_decay=0.001, batch_size=16 ===\n",
      "lr=0.0003, wd=0.001, bs=16 | Epoch 1/5 | Loss: 0.1369 | F1: 0.9898 (P: 0.9967, R: 0.9831)\n",
      "lr=0.0003, wd=0.001, bs=16 | Epoch 2/5 | Loss: 0.0508 | F1: 0.9964 (P: 0.9967, R: 0.9961)\n",
      "lr=0.0003, wd=0.001, bs=16 | Epoch 3/5 | Loss: 0.0300 | F1: 0.9993 (P: 1.0000, R: 0.9987)\n",
      "lr=0.0003, wd=0.001, bs=16 | Epoch 4/5 | Loss: 0.0182 | F1: 0.9993 (P: 1.0000, R: 0.9987)\n",
      "lr=0.0003, wd=0.001, bs=16 | Epoch 5/5 | Loss: 0.0121 | F1: 0.9977 (P: 1.0000, R: 0.9954)\n",
      "\n",
      "=== Training with lr=0.0003, weight_decay=0.001, batch_size=32 ===\n",
      "lr=0.0003, wd=0.001, bs=32 | Epoch 1/5 | Loss: 0.1792 | F1: 0.9825 (P: 0.9960, R: 0.9694)\n",
      "lr=0.0003, wd=0.001, bs=32 | Epoch 2/5 | Loss: 0.0569 | F1: 0.9928 (P: 0.9941, R: 0.9915)\n",
      "lr=0.0003, wd=0.001, bs=32 | Epoch 3/5 | Loss: 0.0328 | F1: 0.9974 (P: 0.9980, R: 0.9967)\n",
      "lr=0.0003, wd=0.001, bs=32 | Epoch 4/5 | Loss: 0.0238 | F1: 0.9974 (P: 0.9987, R: 0.9961)\n",
      "lr=0.0003, wd=0.001, bs=32 | Epoch 5/5 | Loss: 0.0160 | F1: 0.9977 (P: 0.9980, R: 0.9974)\n",
      "\n",
      "=== Training with lr=0.0003, weight_decay=0.001, batch_size=64 ===\n",
      "lr=0.0003, wd=0.001, bs=64 | Epoch 1/5 | Loss: 0.2404 | F1: 0.9804 (P: 0.9849, R: 0.9759)\n",
      "lr=0.0003, wd=0.001, bs=64 | Epoch 2/5 | Loss: 0.0748 | F1: 0.9839 (P: 0.9947, R: 0.9733)\n",
      "lr=0.0003, wd=0.001, bs=64 | Epoch 3/5 | Loss: 0.0504 | F1: 0.9938 (P: 0.9948, R: 0.9928)\n",
      "lr=0.0003, wd=0.001, bs=64 | Epoch 4/5 | Loss: 0.0363 | F1: 0.9951 (P: 0.9954, R: 0.9948)\n",
      "lr=0.0003, wd=0.001, bs=64 | Epoch 5/5 | Loss: 0.0264 | F1: 0.9944 (P: 0.9961, R: 0.9928)\n",
      "\n",
      "=== Training with lr=0.0001, weight_decay=0.0, batch_size=16 ===\n",
      "lr=0.0001, wd=0.0, bs=16 | Epoch 1/5 | Loss: 0.2147 | F1: 0.9788 (P: 0.9953, R: 0.9628)\n",
      "lr=0.0001, wd=0.0, bs=16 | Epoch 2/5 | Loss: 0.0969 | F1: 0.9878 (P: 0.9954, R: 0.9804)\n",
      "lr=0.0001, wd=0.0, bs=16 | Epoch 3/5 | Loss: 0.0663 | F1: 0.9908 (P: 0.9960, R: 0.9857)\n",
      "lr=0.0001, wd=0.0, bs=16 | Epoch 4/5 | Loss: 0.0466 | F1: 0.9944 (P: 0.9961, R: 0.9928)\n",
      "lr=0.0001, wd=0.0, bs=16 | Epoch 5/5 | Loss: 0.0420 | F1: 0.9958 (P: 0.9974, R: 0.9941)\n",
      "\n",
      "=== Training with lr=0.0001, weight_decay=0.0, batch_size=32 ===\n",
      "lr=0.0001, wd=0.0, bs=32 | Epoch 1/5 | Loss: 0.2777 | F1: 0.9758 (P: 0.9809, R: 0.9707)\n",
      "lr=0.0001, wd=0.0, bs=32 | Epoch 2/5 | Loss: 0.1096 | F1: 0.9815 (P: 0.9933, R: 0.9700)\n",
      "lr=0.0001, wd=0.0, bs=32 | Epoch 3/5 | Loss: 0.0796 | F1: 0.9915 (P: 0.9935, R: 0.9896)\n",
      "lr=0.0001, wd=0.0, bs=32 | Epoch 4/5 | Loss: 0.0634 | F1: 0.9918 (P: 0.9948, R: 0.9889)\n",
      "lr=0.0001, wd=0.0, bs=32 | Epoch 5/5 | Loss: 0.0530 | F1: 0.9922 (P: 0.9954, R: 0.9889)\n",
      "\n",
      "=== Training with lr=0.0001, weight_decay=0.0, batch_size=64 ===\n",
      "lr=0.0001, wd=0.0, bs=64 | Epoch 1/5 | Loss: 0.3702 | F1: 0.9654 (P: 0.9767, R: 0.9544)\n",
      "lr=0.0001, wd=0.0, bs=64 | Epoch 2/5 | Loss: 0.1597 | F1: 0.9769 (P: 0.9765, R: 0.9772)\n",
      "lr=0.0001, wd=0.0, bs=64 | Epoch 3/5 | Loss: 0.1118 | F1: 0.9809 (P: 0.9894, R: 0.9726)\n",
      "lr=0.0001, wd=0.0, bs=64 | Epoch 4/5 | Loss: 0.0890 | F1: 0.9846 (P: 0.9901, R: 0.9791)\n",
      "lr=0.0001, wd=0.0, bs=64 | Epoch 5/5 | Loss: 0.0698 | F1: 0.9866 (P: 0.9914, R: 0.9817)\n",
      "\n",
      "=== Training with lr=0.0001, weight_decay=0.0001, batch_size=16 ===\n",
      "lr=0.0001, wd=0.0001, bs=16 | Epoch 1/5 | Loss: 0.2242 | F1: 0.9809 (P: 0.9933, R: 0.9687)\n",
      "lr=0.0001, wd=0.0001, bs=16 | Epoch 2/5 | Loss: 0.0970 | F1: 0.9899 (P: 0.9941, R: 0.9857)\n",
      "lr=0.0001, wd=0.0001, bs=16 | Epoch 3/5 | Loss: 0.0721 | F1: 0.9918 (P: 0.9961, R: 0.9876)\n",
      "lr=0.0001, wd=0.0001, bs=16 | Epoch 4/5 | Loss: 0.0515 | F1: 0.9951 (P: 0.9967, R: 0.9935)\n",
      "lr=0.0001, wd=0.0001, bs=16 | Epoch 5/5 | Loss: 0.0440 | F1: 0.9961 (P: 0.9980, R: 0.9941)\n",
      "\n",
      "=== Training with lr=0.0001, weight_decay=0.0001, batch_size=32 ===\n",
      "lr=0.0001, wd=0.0001, bs=32 | Epoch 1/5 | Loss: 0.2967 | F1: 0.9720 (P: 0.9808, R: 0.9635)\n",
      "lr=0.0001, wd=0.0001, bs=32 | Epoch 2/5 | Loss: 0.1211 | F1: 0.9823 (P: 0.9881, R: 0.9765)\n",
      "lr=0.0001, wd=0.0001, bs=32 | Epoch 3/5 | Loss: 0.0834 | F1: 0.9866 (P: 0.9902, R: 0.9831)\n",
      "lr=0.0001, wd=0.0001, bs=32 | Epoch 4/5 | Loss: 0.0686 | F1: 0.9902 (P: 0.9947, R: 0.9857)\n",
      "lr=0.0001, wd=0.0001, bs=32 | Epoch 5/5 | Loss: 0.0513 | F1: 0.9918 (P: 0.9954, R: 0.9883)\n",
      "\n",
      "=== Training with lr=0.0001, weight_decay=0.0001, batch_size=64 ===\n",
      "lr=0.0001, wd=0.0001, bs=64 | Epoch 1/5 | Loss: 0.3686 | F1: 0.9589 (P: 0.9607, R: 0.9570)\n",
      "lr=0.0001, wd=0.0001, bs=64 | Epoch 2/5 | Loss: 0.1646 | F1: 0.9707 (P: 0.9820, R: 0.9596)\n",
      "lr=0.0001, wd=0.0001, bs=64 | Epoch 3/5 | Loss: 0.1175 | F1: 0.9776 (P: 0.9874, R: 0.9681)\n",
      "lr=0.0001, wd=0.0001, bs=64 | Epoch 4/5 | Loss: 0.0909 | F1: 0.9847 (P: 0.9863, R: 0.9831)\n",
      "lr=0.0001, wd=0.0001, bs=64 | Epoch 5/5 | Loss: 0.0766 | F1: 0.9869 (P: 0.9889, R: 0.9850)\n",
      "\n",
      "=== Training with lr=0.0001, weight_decay=0.001, batch_size=16 ===\n",
      "lr=0.0001, wd=0.001, bs=16 | Epoch 1/5 | Loss: 0.2308 | F1: 0.9826 (P: 0.9894, R: 0.9759)\n",
      "lr=0.0001, wd=0.001, bs=16 | Epoch 2/5 | Loss: 0.0994 | F1: 0.9869 (P: 0.9954, R: 0.9785)\n",
      "lr=0.0001, wd=0.001, bs=16 | Epoch 3/5 | Loss: 0.0681 | F1: 0.9928 (P: 0.9961, R: 0.9896)\n",
      "lr=0.0001, wd=0.001, bs=16 | Epoch 4/5 | Loss: 0.0547 | F1: 0.9931 (P: 0.9980, R: 0.9883)\n",
      "lr=0.0001, wd=0.001, bs=16 | Epoch 5/5 | Loss: 0.0386 | F1: 0.9951 (P: 0.9987, R: 0.9915)\n",
      "\n",
      "=== Training with lr=0.0001, weight_decay=0.001, batch_size=32 ===\n",
      "lr=0.0001, wd=0.001, bs=32 | Epoch 1/5 | Loss: 0.2676 | F1: 0.9708 (P: 0.9892, R: 0.9531)\n",
      "lr=0.0001, wd=0.001, bs=32 | Epoch 2/5 | Loss: 0.1120 | F1: 0.9809 (P: 0.9920, R: 0.9700)\n",
      "lr=0.0001, wd=0.001, bs=32 | Epoch 3/5 | Loss: 0.0822 | F1: 0.9875 (P: 0.9947, R: 0.9804)\n",
      "lr=0.0001, wd=0.001, bs=32 | Epoch 4/5 | Loss: 0.0661 | F1: 0.9892 (P: 0.9960, R: 0.9824)\n",
      "lr=0.0001, wd=0.001, bs=32 | Epoch 5/5 | Loss: 0.0499 | F1: 0.9928 (P: 0.9954, R: 0.9902)\n",
      "\n",
      "=== Training with lr=0.0001, weight_decay=0.001, batch_size=64 ===\n",
      "lr=0.0001, wd=0.001, bs=64 | Epoch 1/5 | Loss: 0.3660 | F1: 0.9624 (P: 0.9733, R: 0.9518)\n",
      "lr=0.0001, wd=0.001, bs=64 | Epoch 2/5 | Loss: 0.1598 | F1: 0.9747 (P: 0.9815, R: 0.9681)\n",
      "lr=0.0001, wd=0.001, bs=64 | Epoch 3/5 | Loss: 0.1129 | F1: 0.9786 (P: 0.9874, R: 0.9700)\n",
      "lr=0.0001, wd=0.001, bs=64 | Epoch 4/5 | Loss: 0.0911 | F1: 0.9819 (P: 0.9888, R: 0.9752)\n",
      "lr=0.0001, wd=0.001, bs=64 | Epoch 5/5 | Loss: 0.0754 | F1: 0.9829 (P: 0.9894, R: 0.9765)\n",
      "\n",
      "=== Best configuration ===\n",
      "{'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 16, 'epoch': 5}\n",
      "Best F1: 1.0000\n",
      "Saved best model to: results/resnet_best_grid.pt\n"
     ]
    }
   ],
   "source": [
    "# import the required libraries and packages\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "# see if cuda is available\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# load dataset in\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dir = \"../data/train\"\n",
    "test_dir  = \"../data/test\"\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "num_epochs = 5\n",
    "\n",
    "learning_rates = [0.001, 0.0003, 0.0001]\n",
    "weight_decays = [0.0, 1e-4, 1e-3]\n",
    "batch_sizes   = [16, 32, 64]\n",
    "\n",
    "best_f1 = -1.0\n",
    "best_config = None\n",
    "best_state_dict = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        for bs in batch_sizes:\n",
    "            print(f\"\\n=== Training with lr={lr}, weight_decay={wd}, batch_size={bs} ===\")\n",
    "\n",
    "            # data loaders\n",
    "            trainloader = torch.utils.data.DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=bs,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "            )\n",
    "\n",
    "            testloader = torch.utils.data.DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=bs,\n",
    "                shuffle=False,\n",
    "                num_workers=4,\n",
    "            )\n",
    "\n",
    "            # model for hyper parameters\n",
    "            weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "            model = models.resnet18(weights=weights)\n",
    "\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "            model = model.to(device)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=lr,\n",
    "                momentum=0.9,\n",
    "                weight_decay=wd,\n",
    "            )\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                # train\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                total = 0\n",
    "\n",
    "                for inputs, labels in trainloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                epoch_loss = running_loss / total\n",
    "\n",
    "                # evaluate\n",
    "                model.eval()\n",
    "                tp = fp = fn = tn = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in testloader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                        # 1 = rotten, 0 = fresh\n",
    "                        tp += ((preds == 1) & (labels == 1)).sum().item()\n",
    "                        fp += ((preds == 1) & (labels == 0)).sum().item()\n",
    "                        fn += ((preds == 0) & (labels == 1)).sum().item()\n",
    "                        tn += ((preds == 0) & (labels == 0)).sum().item()\n",
    "\n",
    "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "                recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "                f1        = (\n",
    "                    2 * precision * recall / (precision + recall)\n",
    "                    if (precision + recall) > 0 else 0.0\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"lr={lr}, wd={wd}, bs={bs} | \"\n",
    "                    f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "                    f\"Loss: {epoch_loss:.4f} | \"\n",
    "                    f\"F1: {f1:.4f} (P: {precision:.4f}, R: {recall:.4f})\"\n",
    "                )\n",
    "\n",
    "                # keep track of best model\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_config = {\n",
    "                        \"lr\": lr,\n",
    "                        \"weight_decay\": wd,\n",
    "                        \"batch_size\": bs,\n",
    "                        \"epoch\": epoch + 1,\n",
    "                    }\n",
    "                    best_state_dict = deepcopy(model.state_dict())\n",
    "\n",
    "# save the best model\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "best_model_path = \"../results/resnet_best_grid.pt\"\n",
    "torch.save(best_state_dict, best_model_path)\n",
    "\n",
    "print(\"\\n=== Best configuration ===\")\n",
    "print(best_config)\n",
    "print(f\"Best F1: {best_f1:.4f}\")\n",
    "print(f\"Saved best model to: {best_model_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
